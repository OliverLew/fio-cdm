#!/usr/bin/env python
import os
import sys
import glob
import json
import argparse
import logging
import configparser
import subprocess

logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))


def readable2byte(num):
    units = {'k': 1, 'm': 2, 'g': 3, 't': 4}
    num = num.lower().rstrip("b").rstrip("i")
    if num[-1] in units.keys():
        return int(num[:-1]) * 1024 ** units[num[-1]]
    elif num[-1].isdigit():
        return int(num)
    else:
        logging.error("Unrecognised unit: %s(i)B. Largest available is TiB",
                      num[-1].upper())
        exit(1)


def byte2readable(num):
    for unit in ['', 'Ki', 'Mi', 'Gi']:
        if abs(num) < 1024.0:
            return "%3.1f%sB" % (num, unit)
        num /= 1024.0
    return "%.1f%sB" % (num, 'Ti')


class Job:
    def __init__(self, size, directory):
        self.jobs = []
        self.bandwidth = {}
        self.config = configparser.ConfigParser(allow_no_value=True)
        self.config.read_dict({
            'global': {
                'ioengine': 'libaio',
                # TODO: time vary with number of jobs
                'runtime': '120',
                'size': size,
                'directory': directory,
                'fdatasync': '1',
                'direct': '1',
                'refill_buffers': None,
                'norandommap': None,
                'randrepeat': '0',
                'allrandrepeat': '0',
                'group_reporting': None
            }
        })

    def _jobname_templ(self, job_dict):
        jobname = "Seq-" if job_dict["rw"] == 'seq' else "Rand-"
        jobname += "{rw}-"
        jobname += "{}".format(job_dict["bs"].upper())
        jobname += "-Q{}".format(job_dict["q"]) if job_dict["q"] > 1 else ""
        jobname += "-T{}".format(job_dict["t"]) if job_dict["t"] > 1 else ""
        return jobname

    def _displayname(self, job_dict):
        displayname = "SEQ" if job_dict["rw"] == 'seq' else "RND"
        displayname += "{} ".format(job_dict["bs"].upper())
        displayname += "Q{:2}".format(str(job_dict["q"]))
        displayname += "T{:2}".format(str(job_dict["t"]))
        return displayname

    def create_jobfile(self, jobfile):
        if jobfile == '-':
            self.config.write(sys.stdout, space_around_delimiters=False)
        else:
            with open(jobfile, 'w') as f:
                self.config.write(f, space_around_delimiters=False)

    def create_job(self, rw_type, queue_size, thread_num):
        blocksize = '1m' if rw_type == 'seq' else '4k'

        job = {
            "rw": rw_type,
            "bs": blocksize,
            "q": queue_size,
            "t": thread_num
        }
        self.jobs.append(job)

        if args.x == 0:
            rw_name_type = [("Read","read"), ("Write", "write")]
        else:
            rw_name_type = [("Read","read"), ("Write", "write"), ("Mix", "rw")]

        for name, rw in rw_name_type:
            self.config.read_dict({
                self._jobname_templ(job).format(rw=name): {
                    'bs': blocksize,
                    'rw': rw if rw_type == 'seq' else 'rand' + rw,
                    'rwmixread': args.x,
                    'iodepth': queue_size,
                    'numjobs': thread_num,
                    'stonewall': None
                }
            })

    def run(self):
        jobfile = 'jobfile'
        if not self._check_disk_space():
            exit(1)

        if args.f:
            self.create_jobfile(args.f)
            exit()
        else:
            self.create_jobfile(jobfile)

        try:
            cmd = ['fio', '--output-format', 'json', jobfile]
            output = subprocess.check_output(cmd)
        except KeyboardInterrupt:
            logging.info('interrupted, cleaning up before exit...')
            exit(1)
        finally:
            self._clean_testfiles()
            os.remove(jobfile)

        logging.debug(output.decode('utf-8'))
        info = json.loads(output)

        # Unit of I/O speed, use MB/s(10^6) instead of MiB/s(2^30).
        unit = 10**6

        # TODO: don't feel right about this way of storing data
        for job in info['jobs']:
            rw = job['job options']['rw']
            if rw == 'read' or rw == 'randread':
                self.bandwidth[job['jobname']] = job['read']['bw_bytes'] / unit
            if rw == 'write' or rw == 'randwrite':
                self.bandwidth[job['jobname']] = job['write']['bw_bytes'] / unit
            if rw == 'rw' or rw == 'randrw':
                self.bandwidth[job['jobname']] = job['write']['bw_bytes'] / unit

    def print_result(self):
        if args.x:
            print("|Name        | Read  | Write |  Mix  |\n"
                  "|------------|-------|-------|-------|")
            template_row = "|{jobname}|{read:7.2f}|{write:7.2f}|{mix:7.2f}|"
        else:
            print("|Name        | Read  | Write |\n"
                  "|------------|-------|-------|")
            template_row = "|{jobname}|{read:7.2f}|{write:7.2f}|"

        for job in self.jobs:
            jobname = self._jobname_templ(job)
            print(template_row.format(
                jobname=self._displayname(job),
                read=self.bandwidth[jobname.format(rw="Read")],
                write=self.bandwidth[jobname.format(rw="Write")],
                mix=self.bandwidth[jobname.format(rw="Mix")] if args.x else 0
            ))

    def _clean_testfiles(self):
        for job in self.jobs:
            jobname = self._jobname_templ(job)
            for rw in ["Read", "Write", "Mix"]:
                wildcard = os.path.join(args.target, jobname.format(rw=rw) + "*")
                for f in glob.glob(wildcard):
                    logging.debug("Removig file %s", f)
                    os.remove(f)

    def _check_disk_space(self):
        statvfs = os.statvfs(args.target)
        avail = statvfs.f_frsize * statvfs.f_bfree
        needed = 0

        for job in self.jobs:
            needed += readable2byte(args.s) * job["t"]
        logging.debug("Target: %s\navailable: %s\nneeded: %s",
                      args.target, byte2readable(avail), byte2readable(needed))
        if avail > needed:
            return True
        else:
            logging.warning("Not enough space available in %s:", args.target)
            logging.warning("Needed: %s. Available: %s",
                            byte2readable(needed), byte2readable(avail))
            return False


def get_parser():
    parser = argparse.ArgumentParser(
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description='A python script to show disk test results with fio',
            epilog='''
Note:
    Do not add argument <target> after -x (as in \"fio-cdm -x <target>\"), \
it will be regarded as <mix> but an error will pop up since -x optionally \
needs an integer.

Examples:
    Set test file size to 512MB, run 5 times with read, write and mix tests \
(specify the mix percentage to be 60%):
        fio-cdm -s 512m -n 5 -x 60

    Manually add jobs:
        fio-cdm -a seq,1,1 seq,32,1 rand,16,8

    Show the equivalent command directly with fio:
        fio-cdm -f - | fio --showcmd -
''')

    parser.add_argument('target',
                        help='The path of the directory to test')
    # TODO: run multiple times
    parser.add_argument('-n', metavar='number',
                        type=int,
                        default=1,
                        help='''Number of tests, default is 1''')
    parser.add_argument('-s', metavar='size',
                        default='1G',
                        help='''The size of file I/O, same as the fio parameter,
                             default is 1G''')
    parser.add_argument('-x', metavar='mix',
                        type=int,
                        nargs="?",
                        const=70,
                        default=0,
                        help='''Add mixed rw test, default is disabled.
                             <mix> is read percentage (integer), default is 70.''')
    parser.add_argument('-a', metavar='job',
                        nargs="+",
                        help='''Manually add multiple jobs. Override default.
                             Format is \"seq|rand,queue depth,thread number\".''')
    parser.add_argument('-f', metavar='dump-jobfile',
                        help='''Save jobfile and quit without running fio.
                             Use \'-\' to print to stdout.''')
    return parser


if __name__ == '__main__':
    # TODO: realtime visual feedback, with fio --debug
    # TODO: time upper limit do this with 'runtime'
    # TODO: show iops or latency
    parser = get_parser()
    args = parser.parse_args()
    logging.debug(args)

    fio_job = Job(args.s, args.target)
    if args.a:
        for job in args.a:
            rw_type, qd, tn = job.split(',')
            fio_job.create_job(rw_type, int(qd), int(tn))
    else:
        fio_job.create_job('seq', 32, 1)
        #  fio_job.create_job('rand', 1, 1)
        #  fio_job.create_job('rand', 16, 1)
        #  fio_job.create_job('rand', 32, 8)
    fio_job.run()
    fio_job.print_result()
